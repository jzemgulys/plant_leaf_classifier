{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a737986f",
   "metadata": {},
   "source": [
    "# Plant Leaf Disease Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65984d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#import os\n",
    "#os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bd5c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9c93de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf_classif\\train\n"
     ]
    }
   ],
   "source": [
    "root = 'leaf_classif'\n",
    "print(os.path.join(root, 'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "032083ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "files = [file for file in os.listdir(root) if os.path.isfile(os.path.join(root, file))]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b03d1",
   "metadata": {},
   "source": [
    "# We extracted the pictures of leaves and gave them their corresponding labels\n",
    "\n",
    "## All of the leaf pictures have the dimensions 6000 x 4000 and 96 dpi for both horizontal and vertical resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c145fb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4274 validated image filenames belonging to 2 classes.\n",
      "Found 110 validated image filenames belonging to 2 classes.\n",
      "Found 110 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "main_dir = root  # Replace with your main directory\n",
    "\n",
    "def label_from_folder_name(folder_name):\n",
    "    if 'healthy' in folder_name:\n",
    "        return 'healthy'  # Class label for healthy images\n",
    "    elif 'diseased' in folder_name:\n",
    "        return 'diseased'  # Class label for unhealthy images\n",
    "    else:\n",
    "        return None  # No specific label found\n",
    "    \n",
    "def custom_flow_from_directory(directory, target_size, batch_size):\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    i = 0\n",
    "\n",
    "    for folder in os.listdir(directory):\n",
    "        # first 2 folders\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for root, dirs, files in os.walk(folder_path):\n",
    "                 for file in files:\n",
    "                     filenames.append(os.path.join(root, file))\n",
    "                     labels.append(label_from_folder_name(root))\n",
    "\n",
    "    filenames = np.array(filenames)\n",
    "    labels = np.array(labels, dtype=str)  # Ensure labels are strings\n",
    "\n",
    "    return ImageDataGenerator(rescale=1./255).flow_from_dataframe(\n",
    "        pd.DataFrame({\"filename\": filenames, \"class\": labels}),\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"class\",\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "\n",
    "# Use the custom function to load data\n",
    "batch_size = 32\n",
    "img_height, img_width = 150, 150\n",
    "\n",
    "train_dir = os.path.join(main_dir, 'train')\n",
    "test_dir = os.path.join(main_dir, 'test')\n",
    "valid_dir = os.path.join(main_dir, 'valid')\n",
    "\n",
    "train_generator = custom_flow_from_directory(train_dir, (img_height, img_width), batch_size)\n",
    "test_generator = custom_flow_from_directory(test_dir, (img_height, img_width), batch_size)\n",
    "valid_generator = custom_flow_from_directory(valid_dir, (img_height, img_width), batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692862ce",
   "metadata": {},
   "source": [
    "# Originally, we tried to run the model on all of the leaves, but found that we do not have the necessary hardware resources to do this.\n",
    "- We tried to use Google Colab, but the service began to limit our usage of GPU and we encountered difficulty in trying to fully run the program.\n",
    "\n",
    "# So, we decided to run the model on one type of leaf - Alstonia Scholaris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80cd6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf8d18",
   "metadata": {},
   "source": [
    "# We used a Convolutional Neural Network (CNN) from the Keras library to build a sequential model.\n",
    "\n",
    "# Convolutional layers\n",
    "- The first convolutional layer (Conv2D) has 32 filters, each with a 3x3 kernel. It applies the Rectified Linear Unit (ReLU) activation function.\n",
    "- The second convolutional layer has 64 filters with a 3x3 kernel and ReLU activation.\n",
    "- The third convolutional layer has 128 filters with a 3x3 kernel and ReLU activation.\n",
    "- After each convolutional layer, a max-pooling layer (MaxPooling2D) with a 2x2 pool size is applied. This reduces the spatial dimensions of the representation and captures the most important information.\n",
    "\n",
    "# Dense layers\n",
    "\n",
    "- The first fully connected layer (Dense) has 128 neurons with ReLU activation. It processes the flattened features from the previous layer.\n",
    "- The second and final fully connected layer has 1 neuron with a sigmoid activation function. This neuron outputs the probability of belonging to the positive class in binary classification tasks.\n",
    "\n",
    "# Model Compilation\n",
    "\n",
    "- The model is compiled using the Adam optimizer ('adam'), a popular optimization algorithm.\n",
    "- The loss function used for training is binary crossentropy ('binary_crossentropy'), suitable for binary classification problems.\n",
    "- The metric to monitor during training is accuracy ('accuracy').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62ba4942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "134/134 [==============================] - 734s 5s/step - loss: 0.6559 - accuracy: 0.6015 - val_loss: 0.6830 - val_accuracy: 0.6273\n",
      "Epoch 2/10\n",
      "134/134 [==============================] - 696s 5s/step - loss: 0.5465 - accuracy: 0.7272 - val_loss: 0.7343 - val_accuracy: 0.6636\n",
      "Epoch 3/10\n",
      "134/134 [==============================] - 697s 5s/step - loss: 0.4536 - accuracy: 0.7883 - val_loss: 0.5778 - val_accuracy: 0.7273\n",
      "Epoch 4/10\n",
      "134/134 [==============================] - 1481s 11s/step - loss: 0.3437 - accuracy: 0.8545 - val_loss: 0.5284 - val_accuracy: 0.7545\n",
      "Epoch 5/10\n",
      "134/134 [==============================] - 1983s 15s/step - loss: 0.2820 - accuracy: 0.8842 - val_loss: 0.5131 - val_accuracy: 0.8182\n",
      "Epoch 6/10\n",
      "134/134 [==============================] - 1979s 15s/step - loss: 0.2355 - accuracy: 0.9073 - val_loss: 0.5761 - val_accuracy: 0.7636\n",
      "Epoch 7/10\n",
      "134/134 [==============================] - 975s 7s/step - loss: 0.2029 - accuracy: 0.9202 - val_loss: 0.4438 - val_accuracy: 0.8364\n",
      "Epoch 8/10\n",
      "134/134 [==============================] - 828s 6s/step - loss: 0.1643 - accuracy: 0.9378 - val_loss: 0.4700 - val_accuracy: 0.8182\n",
      "Epoch 9/10\n",
      "134/134 [==============================] - 808s 6s/step - loss: 0.1500 - accuracy: 0.9413 - val_loss: 0.5567 - val_accuracy: 0.8000\n",
      "Epoch 10/10\n",
      "134/134 [==============================] - 806s 6s/step - loss: 0.1224 - accuracy: 0.9513 - val_loss: 0.5602 - val_accuracy: 0.8364\n"
     ]
    }
   ],
   "source": [
    "# Calculate steps_per_epoch and validation_steps\n",
    "steps_per_epoch_train = train_generator.samples // batch_size\n",
    "steps_per_epoch_valid = valid_generator.samples // batch_size\n",
    "steps_per_epoch_test = test_generator.samples // batch_size\n",
    "\n",
    "epochs = 10\n",
    "# Add 1 extra step if there are remaining samples not included in batches\n",
    "if train_generator.samples % batch_size != 0:\n",
    "    steps_per_epoch_train += 1\n",
    "if valid_generator.samples % batch_size != 0:\n",
    "    steps_per_epoch_valid += 1\n",
    "if test_generator.samples % batch_size != 0:\n",
    "    steps_per_epoch_test += 1\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=steps_per_epoch_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e3c3cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 20s 5s/step - loss: 0.5602 - accuracy: 0.8364\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=steps_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6c9350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features and labels from test generator\n",
    "X_test, Y_test = zip(*[(x, y) for x, y in test_generator])\n",
    "\n",
    "# flatten features\n",
    "X_test_flat = [item for sublist in X_test for item in sublist]\n",
    "\n",
    "\n",
    "# calc permutation importance\n",
    "result = permutation_importance(model, X_test_flat, Y_test, n_repeats=10, random_state=40, n_jobs=-1)\n",
    "feature_importance = result.importances_mean\n",
    "\n",
    "# show feature importance\n",
    "feature_names = [f\"Feature {i}\" for i in range(len(feature_importance))]\n",
    "\n",
    "plt.barh(feature_names, feature_importance)\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b554f86e",
   "metadata": {
    "id": "AbwhUtUgq_16"
   },
   "source": [
    "Fine tune CNN model - doubling number of epochs to 20, reducing steps size by half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8de9312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "67/67 [==============================] - 427s 6s/step - loss: 0.1131 - accuracy: 0.9576 - val_loss: 0.4490 - val_accuracy: 0.8281\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 413s 6s/step - loss: 0.0818 - accuracy: 0.9743 - val_loss: 0.4763 - val_accuracy: 0.8438\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 418s 6s/step - loss: 0.0911 - accuracy: 0.9655 - val_loss: 0.6284 - val_accuracy: 0.7969\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 417s 6s/step - loss: 0.0805 - accuracy: 0.9728 - val_loss: 0.6238 - val_accuracy: 0.8281\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 418s 6s/step - loss: 0.0833 - accuracy: 0.9664 - val_loss: 0.6591 - val_accuracy: 0.8594\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 414s 6s/step - loss: 0.0671 - accuracy: 0.9790 - val_loss: 0.8000 - val_accuracy: 0.8125\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 407s 6s/step - loss: 0.0583 - accuracy: 0.9784 - val_loss: 0.6259 - val_accuracy: 0.8125\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 411s 6s/step - loss: 0.0512 - accuracy: 0.9790 - val_loss: 0.7828 - val_accuracy: 0.8281\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 408s 6s/step - loss: 0.0308 - accuracy: 0.9906 - val_loss: 0.8036 - val_accuracy: 0.8281\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 418s 6s/step - loss: 0.0451 - accuracy: 0.9860 - val_loss: 0.8324 - val_accuracy: 0.8281\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 417s 6s/step - loss: 0.0378 - accuracy: 0.9888 - val_loss: 0.8243 - val_accuracy: 0.8281\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 413s 6s/step - loss: 0.0272 - accuracy: 0.9930 - val_loss: 0.4653 - val_accuracy: 0.8438\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 410s 6s/step - loss: 0.0285 - accuracy: 0.9901 - val_loss: 1.0561 - val_accuracy: 0.8125\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 412s 6s/step - loss: 0.0568 - accuracy: 0.9776 - val_loss: 0.9446 - val_accuracy: 0.7969\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 412s 6s/step - loss: 0.0309 - accuracy: 0.9887 - val_loss: 0.3595 - val_accuracy: 0.8594\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 423s 6s/step - loss: 0.0247 - accuracy: 0.9920 - val_loss: 0.7590 - val_accuracy: 0.8125\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 421s 6s/step - loss: 0.0209 - accuracy: 0.9948 - val_loss: 0.8001 - val_accuracy: 0.8125\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 413s 6s/step - loss: 0.0193 - accuracy: 0.9925 - val_loss: 0.8501 - val_accuracy: 0.8125\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 413s 6s/step - loss: 0.0387 - accuracy: 0.9865 - val_loss: 0.9767 - val_accuracy: 0.8438\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 415s 6s/step - loss: 0.0966 - accuracy: 0.9685 - val_loss: 1.0469 - val_accuracy: 0.7812\n"
     ]
    }
   ],
   "source": [
    "# Calculate steps_per_epoch and validation_steps\n",
    "steps_per_epoch_train = train_generator.samples // (batch_size * 2)\n",
    "steps_per_epoch_valid = valid_generator.samples // (batch_size * 2)\n",
    "steps_per_epoch_test = test_generator.samples // (batch_size * 2)\n",
    "\n",
    "epochs = 20\n",
    "# Add 1 extra step if there are remaining samples not included in batches\n",
    "if train_generator.samples % batch_size != 0:\n",
    "    steps_per_epoch_train += 1\n",
    "if valid_generator.samples % batch_size != 0:\n",
    "    steps_per_epoch_valid += 1\n",
    "if test_generator.samples % batch_size != 0:\n",
    "    steps_per_epoch_test += 1\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=steps_per_epoch_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c921d8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 12s 6s/step - loss: 0.8206 - accuracy: 0.7969\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=steps_per_epoch_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a3cfd3",
   "metadata": {},
   "source": [
    "Use a model built from a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7aed3665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pretrained model\n",
    "from keras.applications import MobileNet\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Define input image size expected by MobileNet\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "# Load the MobileNet model without the top classification layer\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "# Freeze the layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom top layers for binary classification\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)  # Binary classification using sigmoid activation\n",
    "\n",
    "# Combine base model with custom top layers\n",
    "mobileModel = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "mobileModel.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fefa820b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "134/134 [==============================] - 844s 6s/step - loss: 0.0494 - accuracy: 0.9836 - val_loss: 0.8153 - val_accuracy: 0.8091\n",
      "Epoch 2/10\n",
      "134/134 [==============================] - 810s 6s/step - loss: 0.0254 - accuracy: 0.9913 - val_loss: 0.8773 - val_accuracy: 0.8455\n",
      "Epoch 3/10\n",
      "134/134 [==============================] - 814s 6s/step - loss: 0.0142 - accuracy: 0.9956 - val_loss: 0.8577 - val_accuracy: 0.8545\n",
      "Epoch 4/10\n",
      "134/134 [==============================] - 817s 6s/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.9834 - val_accuracy: 0.8364\n",
      "Epoch 5/10\n",
      "134/134 [==============================] - 808s 6s/step - loss: 7.1019e-04 - accuracy: 1.0000 - val_loss: 1.0705 - val_accuracy: 0.8273\n",
      "Epoch 6/10\n",
      "134/134 [==============================] - 1235s 9s/step - loss: 4.6795e-04 - accuracy: 1.0000 - val_loss: 1.1161 - val_accuracy: 0.8364\n",
      "Epoch 7/10\n",
      "134/134 [==============================] - 2011s 15s/step - loss: 3.4555e-04 - accuracy: 1.0000 - val_loss: 1.1541 - val_accuracy: 0.8364\n",
      "Epoch 8/10\n",
      "134/134 [==============================] - 2025s 15s/step - loss: 2.7527e-04 - accuracy: 1.0000 - val_loss: 1.1751 - val_accuracy: 0.8364\n",
      "Epoch 9/10\n",
      "134/134 [==============================] - 1995s 15s/step - loss: 2.2956e-04 - accuracy: 1.0000 - val_loss: 1.2226 - val_accuracy: 0.8364\n",
      "Epoch 10/10\n",
      "134/134 [==============================] - 2016s 15s/step - loss: 1.9286e-04 - accuracy: 1.0000 - val_loss: 1.2447 - val_accuracy: 0.8364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13aae26a3a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# Calculate steps_per_epoch and validation_steps\n",
    "steps_per_epoch_train = train_generator.samples // batch_size\n",
    "steps_per_epoch_valid = valid_generator.samples // batch_size\n",
    "epochs = 10\n",
    "# Add 1 extra step if there are remaining samples not included in batches\n",
    "if train_generator.samples % batch_size != 0:\n",
    "    steps_per_epoch_train += 1\n",
    "if valid_generator.samples % batch_size != 0:\n",
    "    steps_per_epoch_valid += 1\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=steps_per_epoch_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b50b4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 48s 11s/step - loss: 0.8397 - accuracy: 0.5273\n",
      "Test Loss: 0.8397\n",
      "Test Accuracy: 52.73%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = mobileModel.evaluate(test_generator)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e7e4bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "67/67 [==============================] - 1035s 15s/step - loss: 1.6930e-04 - accuracy: 1.0000 - val_loss: 1.3276 - val_accuracy: 0.8125\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 1021s 15s/step - loss: 1.4438e-04 - accuracy: 1.0000 - val_loss: 0.9707 - val_accuracy: 0.8594\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 1025s 15s/step - loss: 1.4007e-04 - accuracy: 1.0000 - val_loss: 0.5036 - val_accuracy: 0.8906\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 1025s 15s/step - loss: 1.4237e-04 - accuracy: 1.0000 - val_loss: 1.0700 - val_accuracy: 0.8906\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 1001s 15s/step - loss: 1.2913e-04 - accuracy: 1.0000 - val_loss: 1.3622 - val_accuracy: 0.8281\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 414s 6s/step - loss: 1.1160e-04 - accuracy: 1.0000 - val_loss: 0.9927 - val_accuracy: 0.8281\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 440s 7s/step - loss: 1.1852e-04 - accuracy: 1.0000 - val_loss: 0.8633 - val_accuracy: 0.8594\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 415s 6s/step - loss: 1.0632e-04 - accuracy: 1.0000 - val_loss: 0.8273 - val_accuracy: 0.8906\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 408s 6s/step - loss: 9.5124e-05 - accuracy: 1.0000 - val_loss: 1.3639 - val_accuracy: 0.8594\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 425s 6s/step - loss: 8.1942e-05 - accuracy: 1.0000 - val_loss: 1.1318 - val_accuracy: 0.8594\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 419s 6s/step - loss: 8.7867e-05 - accuracy: 1.0000 - val_loss: 1.7502 - val_accuracy: 0.7812\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 423s 6s/step - loss: 7.6340e-05 - accuracy: 1.0000 - val_loss: 1.6240 - val_accuracy: 0.8281\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 418s 6s/step - loss: 8.1402e-05 - accuracy: 1.0000 - val_loss: 0.9959 - val_accuracy: 0.8750\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 411s 6s/step - loss: 7.1829e-05 - accuracy: 1.0000 - val_loss: 1.3255 - val_accuracy: 0.8438\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 419s 6s/step - loss: 6.0611e-05 - accuracy: 1.0000 - val_loss: 1.9530 - val_accuracy: 0.7656\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 841s 13s/step - loss: 6.3367e-05 - accuracy: 1.0000 - val_loss: 1.2168 - val_accuracy: 0.8125\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 1032s 15s/step - loss: 6.5117e-05 - accuracy: 1.0000 - val_loss: 1.2973 - val_accuracy: 0.8594\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 11984s 181s/step - loss: 5.3959e-05 - accuracy: 1.0000 - val_loss: 1.4854 - val_accuracy: 0.8438\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 1028s 15s/step - loss: 5.3131e-05 - accuracy: 1.0000 - val_loss: 0.6678 - val_accuracy: 0.8906\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 1031s 15s/step - loss: 5.8984e-05 - accuracy: 1.0000 - val_loss: 1.5840 - val_accuracy: 0.7812\n"
     ]
    }
   ],
   "source": [
    "# Fine tune the pretrained model\n",
    "\n",
    "# Calculate steps_per_epoch and validation_steps\n",
    "steps_per_epoch_train = train_generator.samples // (batch_size * 2)\n",
    "steps_per_epoch_valid = valid_generator.samples // (batch_size * 2)\n",
    "epochs = 20\n",
    "# Add 1 extra step if there are remaining samples not included in batches\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=steps_per_epoch_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d13f9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 47s 11s/step - loss: 0.8397 - accuracy: 0.5273\n",
      "Test Loss: 0.8397\n",
      "Test Accuracy: 52.73%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = mobileModel.evaluate(test_generator)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4609333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "images_to_predict_dir = \"predict\"\n",
    "\n",
    "def predict_images_in_folder(model, folder_path):\n",
    "    predictions = []\n",
    "    image_paths = []\n",
    "    print(folder_path)\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(('.JPG')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                image_paths.append(img_path)\n",
    "                prediction = predict_image(model, img_path)\n",
    "                predictions.append(prediction)\n",
    "    return image_paths, predictions\n",
    "\n",
    "# Function to predict a single image\n",
    "def predict_image(model, img_path):\n",
    "    img = image.load_img(img_path, target_size=(img_height, img_width))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.\n",
    "\n",
    "    prediction = model.predict(img_array)\n",
    "    if prediction < 0.5:\n",
    "        return \"Healthy\"\n",
    "    else:\n",
    "        return \"Unhealthy\"\n",
    "\n",
    "image_paths, predictions = predict_images_in_folder(model, images_to_predict_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0091987e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x0 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_images = len(image_paths)\n",
    "num_cols = 5\n",
    "num_rows = -(-num_images // num_cols)  # Ceiling division\n",
    "\n",
    "plt.figure(figsize=(15, 3 * num_rows))\n",
    "for i, (img_path, prediction) in enumerate(zip(image_paths, predictions)):\n",
    "    plt.subplot(num_rows, num_cols, i + 1)\n",
    "    img = image.load_img(img_path, target_size=(img_height, img_width))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"{os.path.basename(img_path)}\\nPrediction: {prediction}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67830b49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
