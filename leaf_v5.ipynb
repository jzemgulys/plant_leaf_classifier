{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ae2429",
   "metadata": {},
   "source": [
    "# Plant Leaf Disease Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65984d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#import os\n",
    "#os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bd5c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9c93de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf_classif\\train\n"
     ]
    }
   ],
   "source": [
    "root = 'leaf_classif'\n",
    "print(os.path.join(root, 'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "032083ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "files = [file for file in os.listdir(root) if os.path.isfile(os.path.join(root, file))]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f071c999",
   "metadata": {},
   "source": [
    "# We extracted the pictures of leaves and gave them their corresponding labels\n",
    "\n",
    "## All of the leaf pictures have the dimensions 6000 x 4000 and 96 dpi for both horizontal and vertical resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c145fb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4274 validated image filenames belonging to 2 classes.\n",
      "Found 110 validated image filenames belonging to 2 classes.\n",
      "Found 110 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "main_dir = root  # Replace with your main directory\n",
    "\n",
    "def label_from_folder_name(folder_name):\n",
    "    if 'healthy' in folder_name:\n",
    "        return 'healthy'  # Class label for healthy images\n",
    "    elif 'diseased' in folder_name:\n",
    "        return 'diseased'  # Class label for unhealthy images\n",
    "    else:\n",
    "        return None  # No specific label found\n",
    "    \n",
    "def custom_flow_from_directory(directory, target_size, batch_size):\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    i = 0\n",
    "\n",
    "    for folder in os.listdir(directory):\n",
    "        # first 2 folders\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for root, dirs, files in os.walk(folder_path):\n",
    "                 for file in files:\n",
    "                     filenames.append(os.path.join(root, file))\n",
    "                     labels.append(label_from_folder_name(root))\n",
    "\n",
    "    filenames = np.array(filenames)\n",
    "    labels = np.array(labels, dtype=str)  # Ensure labels are strings\n",
    "\n",
    "    return ImageDataGenerator(rescale=1./255).flow_from_dataframe(\n",
    "        pd.DataFrame({\"filename\": filenames, \"class\": labels}),\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"class\",\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "\n",
    "# Use the custom function to load data\n",
    "batch_size = 32\n",
    "img_height, img_width = 150, 150\n",
    "\n",
    "train_dir = os.path.join(main_dir, 'train')\n",
    "test_dir = os.path.join(main_dir, 'test')\n",
    "valid_dir = os.path.join(main_dir, 'valid')\n",
    "\n",
    "train_generator = custom_flow_from_directory(train_dir, (img_height, img_width), batch_size)\n",
    "test_generator = custom_flow_from_directory(test_dir, (img_height, img_width), batch_size)\n",
    "valid_generator = custom_flow_from_directory(valid_dir, (img_height, img_width), batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960e1b20",
   "metadata": {},
   "source": [
    "# Originally, we tried to run the model on all of the leaves, but found that we do not have the necessary hardware resources to do this.\n",
    "- We tried to use Google Colab, but the service began to limit our usage of GPU and we encountered difficulty in trying to fully run the program.\n",
    "\n",
    "# So, we decided to run the model on one type of leaf - Alstonia Scholaris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80cd6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73156fdc",
   "metadata": {},
   "source": [
    "# We used a Convolutional Neural Network (CNN) from the Keras library to build a sequential model.\n",
    "\n",
    "# Convolutional layers\n",
    "- The first convolutional layer (Conv2D) has 32 filters, each with a 3x3 kernel. It applies the Rectified Linear Unit (ReLU) activation function.\n",
    "- The second convolutional layer has 64 filters with a 3x3 kernel and ReLU activation.\n",
    "- The third convolutional layer has 128 filters with a 3x3 kernel and ReLU activation.\n",
    "- After each convolutional layer, a max-pooling layer (MaxPooling2D) with a 2x2 pool size is applied. This reduces the spatial dimensions of the representation and captures the most important information.\n",
    "\n",
    "# Dense layers\n",
    "\n",
    "- The first fully connected layer (Dense) has 128 neurons with ReLU activation. It processes the flattened features from the previous layer.\n",
    "- The second and final fully connected layer has 1 neuron with a sigmoid activation function. This neuron outputs the probability of belonging to the positive class in binary classification tasks.\n",
    "\n",
    "# Model Compilation\n",
    "\n",
    "- The model is compiled using the Adam optimizer ('adam'), a popular optimization algorithm.\n",
    "- The loss function used for training is binary crossentropy ('binary_crossentropy'), suitable for binary classification problems.\n",
    "- The metric to monitor during training is accuracy ('accuracy').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62ba4942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "134/134 [==============================] - 13026s 98s/step - loss: 0.6324 - accuracy: 0.6282 - val_loss: 0.6097 - val_accuracy: 0.6182\n",
      "Epoch 2/10\n",
      "134/134 [==============================] - 774s 6s/step - loss: 0.4991 - accuracy: 0.7569 - val_loss: 0.6354 - val_accuracy: 0.6909\n",
      "Epoch 3/10\n",
      "134/134 [==============================] - 742s 6s/step - loss: 0.3726 - accuracy: 0.8379 - val_loss: 0.4356 - val_accuracy: 0.7818\n",
      "Epoch 4/10\n",
      "134/134 [==============================] - 763s 6s/step - loss: 0.2597 - accuracy: 0.8924 - val_loss: 0.6547 - val_accuracy: 0.8091\n",
      "Epoch 5/10\n",
      "134/134 [==============================] - 799s 6s/step - loss: 0.1885 - accuracy: 0.9242 - val_loss: 0.3928 - val_accuracy: 0.8273\n",
      "Epoch 6/10\n",
      "134/134 [==============================] - 786s 6s/step - loss: 0.1707 - accuracy: 0.9321 - val_loss: 0.3341 - val_accuracy: 0.9000\n",
      "Epoch 7/10\n",
      "134/134 [==============================] - 5203s 39s/step - loss: 0.1102 - accuracy: 0.9572 - val_loss: 0.3415 - val_accuracy: 0.8727\n",
      "Epoch 8/10\n",
      "134/134 [==============================] - 2052s 15s/step - loss: 0.0768 - accuracy: 0.9710 - val_loss: 0.3599 - val_accuracy: 0.8636\n",
      "Epoch 9/10\n",
      "134/134 [==============================] - 2066s 15s/step - loss: 0.0586 - accuracy: 0.9775 - val_loss: 0.3774 - val_accuracy: 0.9000\n",
      "Epoch 10/10\n",
      "134/134 [==============================] - 2083s 16s/step - loss: 0.0390 - accuracy: 0.9855 - val_loss: 0.6454 - val_accuracy: 0.8727\n"
     ]
    }
   ],
   "source": [
    "# Calculate steps_per_epoch and validation_steps\n",
    "steps_per_epoch_train = train_generator.samples // batch_size\n",
    "steps_per_epoch_valid = valid_generator.samples // batch_size\n",
    "steps_per_epoch_test = test_generator.samples // batch_size\n",
    "\n",
    "epochs = 10\n",
    "# Add 1 extra step if there are remaining samples not included in batches\n",
    "if train_generator.samples % batch_size != 0:\n",
    "    steps_per_epoch_train += 1\n",
    "if valid_generator.samples % batch_size != 0:\n",
    "    steps_per_epoch_valid += 1\n",
    "if test_generator.samples % batch_size != 0:\n",
    "    steps_per_epoch_test += 1\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=steps_per_epoch_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e3c3cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 52s 12s/step - loss: 0.5944 - accuracy: 0.8545\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=steps_per_epoch_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b554f86e",
   "metadata": {
    "id": "AbwhUtUgq_16"
   },
   "source": [
    "# Fine tune CNN model - doubling number of epochs to 20, reducing steps size by half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8de9312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "67/67 [==============================] - 1067s 16s/step - loss: 0.0296 - accuracy: 0.9907 - val_loss: 0.3337 - val_accuracy: 0.8594\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - 1077s 16s/step - loss: 0.0295 - accuracy: 0.9907 - val_loss: 0.6129 - val_accuracy: 0.8438\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - 1050s 16s/step - loss: 0.0160 - accuracy: 0.9949 - val_loss: 0.7386 - val_accuracy: 0.8906\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - 438s 7s/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 0.6080 - val_accuracy: 0.8750\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - 434s 6s/step - loss: 0.0299 - accuracy: 0.9901 - val_loss: 0.8184 - val_accuracy: 0.8438\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - 417s 6s/step - loss: 0.0241 - accuracy: 0.9920 - val_loss: 0.6099 - val_accuracy: 0.8750\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - 411s 6s/step - loss: 0.0166 - accuracy: 0.9939 - val_loss: 0.4986 - val_accuracy: 0.9062\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - 419s 6s/step - loss: 0.0103 - accuracy: 0.9977 - val_loss: 0.7018 - val_accuracy: 0.8906\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - 448s 7s/step - loss: 0.0217 - accuracy: 0.9925 - val_loss: 0.8901 - val_accuracy: 0.8438\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - 474s 7s/step - loss: 0.0168 - accuracy: 0.9958 - val_loss: 0.7399 - val_accuracy: 0.8594\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - 430s 6s/step - loss: 0.0077 - accuracy: 0.9981 - val_loss: 0.8914 - val_accuracy: 0.8750\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - 419s 6s/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.6661 - val_accuracy: 0.9062\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - 442s 7s/step - loss: 0.0162 - accuracy: 0.9953 - val_loss: 0.8086 - val_accuracy: 0.8594\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - 441s 7s/step - loss: 0.0086 - accuracy: 0.9981 - val_loss: 1.2218 - val_accuracy: 0.7969\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - 438s 7s/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5279 - val_accuracy: 0.8750\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - 431s 6s/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.8003 - val_accuracy: 0.8906\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - 431s 6s/step - loss: 7.6091e-04 - accuracy: 1.0000 - val_loss: 0.7918 - val_accuracy: 0.8906\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - 443s 7s/step - loss: 4.0242e-04 - accuracy: 1.0000 - val_loss: 0.5617 - val_accuracy: 0.9219\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - 559s 8s/step - loss: 6.4410e-04 - accuracy: 1.0000 - val_loss: 1.0438 - val_accuracy: 0.8750\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - 1043s 16s/step - loss: 2.5440e-04 - accuracy: 1.0000 - val_loss: 0.9814 - val_accuracy: 0.8594\n"
     ]
    }
   ],
   "source": [
    "# Calculate steps_per_epoch and validation_steps\n",
    "steps_per_epoch_train = train_generator.samples // (batch_size * 2)\n",
    "steps_per_epoch_valid = valid_generator.samples // (batch_size * 2)\n",
    "steps_per_epoch_test = test_generator.samples // (batch_size * 2)\n",
    "\n",
    "epochs = 20\n",
    "# Add 1 extra step if there are remaining samples not included in batches\n",
    "if train_generator.samples % batch_size != 0:\n",
    "    steps_per_epoch_train += 1\n",
    "if valid_generator.samples % batch_size != 0:\n",
    "    steps_per_epoch_valid += 1\n",
    "if test_generator.samples % batch_size != 0:\n",
    "    steps_per_epoch_test += 1\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=steps_per_epoch_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeba4e2",
   "metadata": {},
   "source": [
    "# This fine-tuned model had slightly worse accuracy than the previous model.\n",
    "\n",
    "# Perhaps this can be attributed to over-fitting introduced by further tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c921d8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 27s 14s/step - loss: 0.7436 - accuracy: 0.8906\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=steps_per_epoch_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a3cfd3",
   "metadata": {},
   "source": [
    "Use a model built from a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aed3665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pretrained model\n",
    "from keras.applications import MobileNet\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Define input image size expected by MobileNet\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "# Load the MobileNet model without the top classification layer\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "# Freeze the layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom top layers for binary classification\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)  # Binary classification using sigmoid activation\n",
    "\n",
    "# Combine base model with custom top layers\n",
    "mobileModel = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "mobileModel.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fefa820b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "134/134 [==============================] - 1822s 14s/step - loss: 0.4102 - accuracy: 0.8119 - val_loss: 0.5509 - val_accuracy: 0.7727\n",
      "Epoch 2/10\n",
      "134/134 [==============================] - 1878s 14s/step - loss: 0.2653 - accuracy: 0.8830 - val_loss: 0.4834 - val_accuracy: 0.7727\n",
      "Epoch 3/10\n",
      "134/134 [==============================] - 1911s 14s/step - loss: 0.2275 - accuracy: 0.9003 - val_loss: 0.5646 - val_accuracy: 0.7636\n",
      "Epoch 4/10\n",
      "134/134 [==============================] - 1925s 14s/step - loss: 0.1840 - accuracy: 0.9249 - val_loss: 0.4640 - val_accuracy: 0.8091\n",
      "Epoch 5/10\n",
      "134/134 [==============================] - 1259s 9s/step - loss: 0.1616 - accuracy: 0.9378 - val_loss: 0.4305 - val_accuracy: 0.8182\n",
      "Epoch 6/10\n",
      "134/134 [==============================] - 744s 6s/step - loss: 0.1423 - accuracy: 0.9420 - val_loss: 0.4693 - val_accuracy: 0.8455\n",
      "Epoch 7/10\n",
      "134/134 [==============================] - 855s 6s/step - loss: 0.1129 - accuracy: 0.9569 - val_loss: 0.4209 - val_accuracy: 0.8455\n",
      "Epoch 8/10\n",
      "134/134 [==============================] - 853s 6s/step - loss: 0.0969 - accuracy: 0.9679 - val_loss: 0.4072 - val_accuracy: 0.8636\n",
      "Epoch 9/10\n",
      "134/134 [==============================] - 860s 6s/step - loss: 0.0768 - accuracy: 0.9736 - val_loss: 0.4685 - val_accuracy: 0.8091\n",
      "Epoch 10/10\n",
      "134/134 [==============================] - 857s 6s/step - loss: 0.0536 - accuracy: 0.9867 - val_loss: 0.4371 - val_accuracy: 0.8455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fa9b0e7fd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# Calculate steps_per_epoch and validation_steps\n",
    "steps_per_epoch_train = train_generator.samples // batch_size\n",
    "steps_per_epoch_valid = valid_generator.samples // batch_size\n",
    "epochs = 10\n",
    "# Add 1 extra step if there are remaining samples not included in batches\n",
    "if train_generator.samples % batch_size != 0:\n",
    "    steps_per_epoch_train += 1\n",
    "if valid_generator.samples % batch_size != 0:\n",
    "    steps_per_epoch_valid += 1\n",
    "\n",
    "mobileModel.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=steps_per_epoch_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf08c0d",
   "metadata": {},
   "source": [
    "# The pretrained model performed poorly. The accuracy was about chance-level.\n",
    "- The pretrained model did not seem to fit our specific classification problem well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b50b4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 21s 5s/step - loss: 0.3049 - accuracy: 0.8909\n",
      "Test Loss: 0.3049\n",
      "Test Accuracy: 89.09%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = mobileModel.evaluate(test_generator)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e7e4bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "66/66 [==============================] - 434s 7s/step - loss: 0.0426 - accuracy: 0.9914 - val_loss: 0.5282 - val_accuracy: 0.8438\n",
      "Epoch 2/20\n",
      "66/66 [==============================] - 421s 6s/step - loss: 0.0358 - accuracy: 0.9952 - val_loss: 0.9526 - val_accuracy: 0.6875\n",
      "Epoch 3/20\n",
      "66/66 [==============================] - 424s 6s/step - loss: 0.0368 - accuracy: 0.9938 - val_loss: 0.6260 - val_accuracy: 0.8438\n",
      "Epoch 4/20\n",
      "66/66 [==============================] - 432s 7s/step - loss: 0.0320 - accuracy: 0.9924 - val_loss: 0.5312 - val_accuracy: 0.7812\n",
      "Epoch 5/20\n",
      "66/66 [==============================] - 428s 6s/step - loss: 0.0260 - accuracy: 0.9972 - val_loss: 0.4978 - val_accuracy: 0.8750\n",
      "Epoch 6/20\n",
      "66/66 [==============================] - 431s 7s/step - loss: 0.0269 - accuracy: 0.9953 - val_loss: 0.2949 - val_accuracy: 0.8438\n",
      "Epoch 7/20\n",
      "66/66 [==============================] - 426s 6s/step - loss: 0.0218 - accuracy: 0.9967 - val_loss: 0.3976 - val_accuracy: 0.9062\n",
      "Epoch 8/20\n",
      "66/66 [==============================] - 421s 6s/step - loss: 0.0268 - accuracy: 0.9943 - val_loss: 0.5773 - val_accuracy: 0.8438\n",
      "Epoch 9/20\n",
      "66/66 [==============================] - 423s 6s/step - loss: 0.0218 - accuracy: 0.9948 - val_loss: 0.1242 - val_accuracy: 0.9375\n",
      "Epoch 10/20\n",
      "66/66 [==============================] - 421s 6s/step - loss: 0.0163 - accuracy: 0.9981 - val_loss: 0.5165 - val_accuracy: 0.8125\n",
      "Epoch 11/20\n",
      "66/66 [==============================] - 426s 6s/step - loss: 0.0117 - accuracy: 0.9995 - val_loss: 0.4352 - val_accuracy: 0.8750\n",
      "Epoch 12/20\n",
      "66/66 [==============================] - 429s 7s/step - loss: 0.0118 - accuracy: 0.9981 - val_loss: 0.3259 - val_accuracy: 0.9375\n",
      "Epoch 13/20\n",
      "66/66 [==============================] - 427s 6s/step - loss: 0.0126 - accuracy: 0.9986 - val_loss: 1.2775 - val_accuracy: 0.7188\n",
      "Epoch 14/20\n",
      "66/66 [==============================] - 426s 6s/step - loss: 0.0095 - accuracy: 0.9990 - val_loss: 0.5118 - val_accuracy: 0.8125\n",
      "Epoch 15/20\n",
      "66/66 [==============================] - 426s 6s/step - loss: 0.0069 - accuracy: 0.9995 - val_loss: 0.5004 - val_accuracy: 0.7812\n",
      "Epoch 16/20\n",
      "66/66 [==============================] - 432s 7s/step - loss: 0.0168 - accuracy: 0.9971 - val_loss: 0.4764 - val_accuracy: 0.8750\n",
      "Epoch 17/20\n",
      "66/66 [==============================] - 429s 6s/step - loss: 0.0147 - accuracy: 0.9981 - val_loss: 0.8829 - val_accuracy: 0.8125\n",
      "Epoch 18/20\n",
      "66/66 [==============================] - 427s 6s/step - loss: 0.0091 - accuracy: 0.9995 - val_loss: 1.1359 - val_accuracy: 0.7812\n",
      "Epoch 19/20\n",
      "66/66 [==============================] - 431s 7s/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.6406 - val_accuracy: 0.8438\n",
      "Epoch 20/20\n",
      "66/66 [==============================] - 433s 7s/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.5465 - val_accuracy: 0.9062\n"
     ]
    }
   ],
   "source": [
    "# Fine tune the pretrained model\n",
    "\n",
    "# Calculate steps_per_epoch and validation_steps\n",
    "steps_per_epoch_train = train_generator.samples // (batch_size * 2)\n",
    "steps_per_epoch_valid = valid_generator.samples // (batch_size * 2)\n",
    "epochs = 20\n",
    "# Add 1 extra step if there are remaining samples not included in batches\n",
    "history = mobileModel.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=steps_per_epoch_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a76e29",
   "metadata": {},
   "source": [
    "# Fine-tuning the model did not improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d13f9bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 20s 5s/step - loss: 0.4559 - accuracy: 0.9000\n",
      "Test Loss: 0.4559\n",
      "Test Accuracy: 90.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = mobileModel.evaluate(test_generator)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa509c6",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## Our best model had an accuracy of 83.64% on the test set\n",
    "\n",
    "## This best model is overfitting the training dataset. Most of the error in our model can likely be attributed to overfitting the training dataset, as it reaches 100% accuracy in the training sets quickly.\n",
    "\n",
    "## Another factor that could be causing non-optimal accuracy is the low image resolution, since the original images were too large and we had to scale them down in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde1bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
